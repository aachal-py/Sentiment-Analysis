{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fb241e7",
   "metadata": {},
   "source": [
    "## Import required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ad344c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2bbcdaf",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3193e12",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  One of the other reviewers has mentioned that ...  positive\n",
       "1  A wonderful little production. <br /><br />The...  positive"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"IMDB Dataset.csv\")\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f5aa4b9",
   "metadata": {},
   "source": [
    "## Split Data in train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "243d5ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train,x_test,y_train,y_test = train_test_split(df.review,df.sentiment,test_size=0.25,random_state=42,stratify=df.sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b41bdf",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3eccc1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(data):\n",
    "    tokens = word_tokenize(data)\n",
    "    return tokens\n",
    "x_train_tokens = x_train.apply(tokenization)\n",
    "x_test_tokens  = x_test.apply(tokenization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "01f3dd16",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46506    [do, n't, see, this, ., this, was, one, of, th...\n",
       "36513    [This, movie, could, have, been, great, ., It,...\n",
       "12376    [I, sort, of, liked, this, movie, ,, not, a, g...\n",
       "21915    [This, movie, could, have, been, so, much, bet...\n",
       "30272    [It, seems, that, it, is, becoming, fashionabl...\n",
       "                               ...                        \n",
       "28091    [This, Documentary, (, Now, available, free, o...\n",
       "5474     [Poor, Basil, Rathbone, ,, an, egotistical, co...\n",
       "43110    [In, April, of, 1965, ,, CBS, broadcast, the, ...\n",
       "5125     [Having, seen, other, Bollywood, flicks, with,...\n",
       "42868    [Brought, to, you, by, the, following, among, ...\n",
       "Name: review, Length: 12500, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123568a5",
   "metadata": {},
   "source": [
    "## Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1462f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleaning(data):\n",
    "    clean_text = [i for i in data if (i not in punctuation) and (i.isalpha()) and (len(i)>1)]\n",
    "    return clean_text\n",
    "\n",
    "x_train_clean = x_train_tokens.apply(cleaning)\n",
    "x_test_clean  = x_test_tokens.apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcd3906c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46506    [do, see, this, this, was, one, of, the, dumbe...\n",
       "36513    [This, movie, could, have, been, great, It, is...\n",
       "12376    [sort, of, liked, this, movie, not, good, one,...\n",
       "21915    [This, movie, could, have, been, so, much, bet...\n",
       "30272    [It, seems, that, it, is, becoming, fashionabl...\n",
       "                               ...                        \n",
       "28091    [This, Documentary, Now, available, free, on, ...\n",
       "5474     [Poor, Basil, Rathbone, an, egotistical, compo...\n",
       "43110    [In, April, of, CBS, broadcast, the, first, of...\n",
       "5125     [Having, seen, other, Bollywood, flicks, with,...\n",
       "42868    [Brought, to, you, by, the, following, among, ...\n",
       "Name: review, Length: 12500, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test_clean"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25d998e9",
   "metadata": {},
   "source": [
    "## Convert data to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9ddb6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_normal(data) : \n",
    "    text = [i.lower() for i in data ]\n",
    "    return text \n",
    "\n",
    "x_train_lower = x_train_clean.apply(text_normal)\n",
    "x_test_lower  = x_test_clean.apply(text_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "84e55083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17949    [just, saw, adam, had, four, sons, for, the, f...\n",
       "5786     [for, one, have, shamelessly, enjoyed, every, ...\n",
       "42175    [this, movie, is, journey, through, the, mind,...\n",
       "39484    [this, was, absolutely, one, of, the, best, mo...\n",
       "34209    [oh, geez, there, are, so, many, other, films,...\n",
       "                               ...                        \n",
       "1950     [besides, the, fact, that, my, list, of, favor...\n",
       "22917    [the, first, and, only, time, saw, shades, was...\n",
       "47481    [this, was, such, waste, of, time, danger, if,...\n",
       "35597    [this, is, by, far, the, most, pathetic, movie...\n",
       "27491    [this, movie, forever, left, an, impression, o...\n",
       "Name: review, Length: 37500, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_lower"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9a13cc",
   "metadata": {},
   "source": [
    "## Stopwords removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50f02162",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = stopwords.words('english')\n",
    "def Stopword_Removal(data):\n",
    "    text = [ i for i in data if i not in stop]\n",
    "    return text \n",
    "x_train_withoutstop = x_train_lower.apply(Stopword_Removal)\n",
    "x_test_withoutstop  = x_test_lower.apply(Stopword_Removal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "917dfa56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17949    [saw, adam, four, sons, first, time, thing, st...\n",
       "5786     [one, shamelessly, enjoyed, every, episode, pu...\n",
       "42175    [movie, journey, mind, screenwriter, caught, p...\n",
       "39484    [absolutely, one, best, movies, seen, br, br, ...\n",
       "34209    [oh, geez, many, films, want, see, got, stuck,...\n",
       "                               ...                        \n",
       "1950     [besides, fact, list, favorite, movie, makers,...\n",
       "22917    [first, time, saw, shades, sneakpreview, even,...\n",
       "47481    [waste, time, danger, watch, tempted, tear, dv...\n",
       "35597    [far, pathetic, movie, indian, cinema, cinema,...\n",
       "27491    [movie, forever, left, impression, watched, fr...\n",
       "Name: review, Length: 37500, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_withoutstop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7a5ace",
   "metadata": {},
   "source": [
    "## Lemmatization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b4b75fe6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(data):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    l1 = []\n",
    "    for i in data :\n",
    "        text = lemma.lemmatize(i)\n",
    "        l1.append(text)\n",
    "    return l1\n",
    "\n",
    "x_train_lemma = x_train_withoutstop.apply(lemmatization)\n",
    "x_test_lemma  = x_test_withoutstop.apply(lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c5efff95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17949    [saw, adam, four, son, first, time, thing, str...\n",
       "5786     [one, shamelessly, enjoyed, every, episode, pu...\n",
       "42175    [movie, journey, mind, screenwriter, caught, p...\n",
       "39484    [absolutely, one, best, movie, seen, br, br, e...\n",
       "34209    [oh, geez, many, film, want, see, got, stuck, ...\n",
       "                               ...                        \n",
       "1950     [besides, fact, list, favorite, movie, maker, ...\n",
       "22917    [first, time, saw, shade, sneakpreview, even, ...\n",
       "47481    [waste, time, danger, watch, tempted, tear, dv...\n",
       "35597    [far, pathetic, movie, indian, cinema, cinema,...\n",
       "27491    [movie, forever, left, impression, watched, fr...\n",
       "Name: review, Length: 37500, dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e84c3fb",
   "metadata": {},
   "source": [
    "## Concatenating cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61381382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_list(data):\n",
    "    text = \" \".join(data)\n",
    "    return text\n",
    "x_train_final = x_train_lemma.apply(join_list)\n",
    "x_test_final  = x_test_lemma.apply(join_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fb1a1579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17949    saw adam four son first time thing struck beli...\n",
       "5786     one shamelessly enjoyed every episode pushing ...\n",
       "42175    movie journey mind screenwriter caught paradox...\n",
       "39484    absolutely one best movie seen br br excellent...\n",
       "34209    oh geez many film want see got stuck nephew we...\n",
       "                               ...                        \n",
       "1950     besides fact list favorite movie maker stanley...\n",
       "22917    first time saw shade sneakpreview even premier...\n",
       "47481    waste time danger watch tempted tear dvd wall ...\n",
       "35597    far pathetic movie indian cinema cinema come t...\n",
       "27491    movie forever left impression watched freshman...\n",
       "Name: review, Length: 37500, dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf58163",
   "metadata": {},
   "source": [
    "## Text to Number "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "61395a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(lowercase=True,stop_words='english',max_df=0.95,max_features=1000)\n",
    "count_train = cv.fit_transform(x_train_final)\n",
    "count_test = cv.transform(x_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "87f07437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<37500x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 1671413 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a89fb33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<12500x1000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 559184 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f9b2ecf6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\anaconda\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>accent</th>\n",
       "      <th>act</th>\n",
       "      <th>acted</th>\n",
       "      <th>acting</th>\n",
       "      <th>action</th>\n",
       "      <th>actor</th>\n",
       "      <th>actress</th>\n",
       "      <th>...</th>\n",
       "      <th>written</th>\n",
       "      <th>wrong</th>\n",
       "      <th>wrote</th>\n",
       "      <th>yeah</th>\n",
       "      <th>year</th>\n",
       "      <th>yes</th>\n",
       "      <th>york</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>zombie</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   ability  able  absolutely  accent  act  acted  acting  action  actor  \\\n",
       "0        0     0           0       0    0      0       0       0      0   \n",
       "1        0     0           0       0    0      0       0       0      0   \n",
       "2        0     0           0       0    0      0       0       0      0   \n",
       "3        0     0           1       0    0      0       0       0      0   \n",
       "4        0     0           0       0    0      0       0       0      0   \n",
       "5        0     0           0       0    0      0       0       0      0   \n",
       "6        0     0           0       0    0      0       0       0      0   \n",
       "7        0     1           0       0    1      1       0       0      0   \n",
       "8        0     0           0       1    0      0       0       0      0   \n",
       "9        0     0           0       0    0      0       0       0      0   \n",
       "\n",
       "   actress  ...  written  wrong  wrote  yeah  year  yes  york  young  younger  \\\n",
       "0        0  ...        0      0      0     0     1    0     0      0        0   \n",
       "1        0  ...        0      0      0     0     0    0     0      0        0   \n",
       "2        0  ...        0      0      0     0     0    0     0      0        0   \n",
       "3        0  ...        0      0      0     0     0    0     0      2        0   \n",
       "4        0  ...        0      0      0     1     0    0     0      0        0   \n",
       "5        0  ...        0      0      0     0     0    0     0      0        0   \n",
       "6        0  ...        0      0      0     0     0    0     0      0        0   \n",
       "7        0  ...        0      0      0     0     0    0     0      0        0   \n",
       "8        0  ...        0      0      0     0     0    0     0      0        0   \n",
       "9        0  ...        0      0      0     0     0    0     0      0        0   \n",
       "\n",
       "   zombie  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  \n",
       "5       0  \n",
       "6       0  \n",
       "7       0  \n",
       "8       0  \n",
       "9       0  \n",
       "\n",
       "[10 rows x 1000 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(count_train.A,columns = cv.get_feature_names())\n",
    "df1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae27991f",
   "metadata": {},
   "source": [
    "## Model Building Using Naive Bayes algoritham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b573adfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnb = MultinomialNB()\n",
    "mnb.fit(count_train.A,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6fa0fdb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['negative', 'negative', 'negative', ..., 'positive', 'positive',\n",
       "       'negative'], dtype='<U8')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_mnb = mnb.predict(count_test.A)\n",
    "pred_mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "98b7f341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['positive', 'positive', 'positive', ..., 'negative', 'negative',\n",
       "       'negative'], dtype='<U8')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred_mnb = mnb.predict(count_train.A)\n",
    "train_pred_mnb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8406a952",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8281333333333334"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_train,train_pred_mnb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0c7a446e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.82      0.83      6250\n",
      "    positive       0.82      0.83      0.83      6250\n",
      "\n",
      "    accuracy                           0.83     12500\n",
      "   macro avg       0.83      0.83      0.83     12500\n",
      "weighted avg       0.83      0.83      0.83     12500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, pred_mnb))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
